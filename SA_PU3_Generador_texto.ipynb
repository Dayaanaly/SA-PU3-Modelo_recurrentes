{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dayaanaly/SA-PU3-Modelo_recurrentes/blob/main/SA_PU3_Generador_texto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Práctica #3: Modelo para generación de texto**\n",
        "\n",
        "*Centro Universitario de Ciencias Exactas e Ingenierías*\n",
        "\n",
        "*División de Tecnologías para la Integración Ciber-Humana*\n",
        "\n",
        "*Ingeniería Biomédica*\n",
        "\n",
        "<br>\n",
        "\n",
        "*Mtra. Sofía Alejandra Aguilar Valdez*\n",
        "\n",
        "14 de octubre de 2022"
      ],
      "metadata": {
        "id": "nVx-kou4pjFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Información del equipo**\n",
        "\n",
        "```NOMBRES:```\n",
        "\n",
        "Esteban Gómez Fuentes\n",
        "\n",
        "Christopher William Sanchez Rodriguez\n",
        "\n",
        "Dayana Analy Pacheco Bañuelos\n",
        "\n",
        "\n",
        "```CÓDIGOS:```\n",
        "\n",
        "217593587\n",
        "\n",
        "217535226\n",
        "\n",
        "215658088\n",
        "\n",
        "```LINK REPOSITORIO:```\n",
        "https://github.com/Dayaanaly/SA-PU3-Modelo_recurrentes\n",
        "\n"
      ],
      "metadata": {
        "id": "dQNzG9Wm4ZQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Contenido**\n",
        "\n",
        "\n",
        "\n",
        "1.   Resumen\n",
        "2.   Marco teórico\n",
        "3.   Objetivos\n",
        "4.   Materiales y métodos\n",
        "5.   Resultados\n",
        "6.   Discusión\n",
        "7.   Conclusiones\n",
        "8.   Referencias\n",
        "\n"
      ],
      "metadata": {
        "id": "QQ_tQMMJpude"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Resumen**\n",
        "Esta práctica consiste en la creación de una red neuronal recurrente (RNN) que sea capaz de procesar el texto del libro “Frankenstein” para utilizarlo como base de datos. En base a estos datos, el modelo debe de generar un párrafo de texto que en teoría debe de tener cierta relación con el contenido del libro. En cuanto a los resultados lo que se busca primordialmente es que el texto generado contenga palabras con coherencia y que formen parte de nuestro diccionario. La redacción, coherencia y lógica de este texto generado queda en un segundo plano, ya que esos son elementos un poco más complejos de obtener, y en cierta medida nuestro modelo de red no tiene forma de interpretar estos parámetros."
      ],
      "metadata": {
        "id": "anvzyOk06GH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Marco teórico**\n",
        "(300-800 palabras)\n",
        "\n",
        "Una Red de Neuronas Artificial (RNA) es un modelo matemático de procesamiento, distribuido paralelamente, en forma de grafo dirigido, simulando el comportamiento biológico de las neuronas y la estructura del cerebro. \n",
        "\n",
        "Las Redes de Neuronas Recurrentes o en inglés, Recurrent Neural Networks (RNN) son modelos de redes de neuronas artificiales (RNAs), donde las conexiones entre unidades forman un ciclo dirigido. Específicamente, un ciclo dirigido es una secuencia en la que la caminata a lo largo de los vértices y bordes está completamente determinada por el conjunto de bordes utilizados y, por lo tanto, tiene una apariencia de un orden específico. Este tipo de redes neuronales está especializado en el procesamiento de datos secuenciales o series temporales como es el caso del reconocimiento de voz y escritura. \n",
        "\n",
        "![](https://www.simplilearn.com/ice9/free_resources_article_thumb/Simple_Recurrent_Neural_Network.png) \n",
        "\n",
        "Una RNN basada en caracteres y de esta manera también podemos usar el caso al mismo tiempo para mostrar el uso de datos de texto. En este ejemplo se entrena un modelo de red neuronal para predecir el siguiente carácter a partir de una secuencia de carácteres. Con este modelo intencionadamente simple, se consigue generar secuencias de texto más largas llamando al modelo repetidamente.\n",
        "\n",
        "El reconocimiento de texto es una de las partes más importantes de una RNN, ya que partir de esta característica y la creatividad, podemos obtener la generación y de algún modo, puede ser considerada un paso previo.  \n",
        "\n",
        "El texto se le conoce al conjunto de frases y palabras coherentes y ordenadas que permiten ser interpretadas y transmiten las ideas de un autor. Su extensión es variable y corresponde a un todo comprensible que tiene una finalidad comunicativa en un contexto dado.  \n",
        "\n",
        "La generación se puede definir como la acción que consiste en producir o crear una cosa, y si la aplicamos a términos de la inteligencia artificial (IA), podría describirse como la capacidad de crear contenido indistinguible del aprendido (original) de manera global pero diferente a cada elemento de manera individual. Un ejemplo fácil en el campo de la visión Artificial sería poder “dibujar” un perro en un campo de hierba, habiendo visto únicamente campos de hierba y perros sobre arena por separado. Si juntáramos las tres imágenes en una misma carpeta, no se podría decir con certeza cuál de las tres ha sido generada por un sistema (ficticio) de creación de imágenes.  \n",
        "\n",
        "Para la creación de texto gramaticalmente correcto en el mismo dominio del original de manera global, pero con diferente significado o contenido a cada uno de los textos de manera individual. Se quiere una técnica de generación de texto fácil de ajustar y que combine una técnica no compleja de clasificación con una búsqueda sobre las soluciones del anterior, consiguiendo la unión de diferentes datos secuenciales relativamente sencillos en un sistema de mayor complejidad.  \n",
        "Un algoritmo de búsqueda debería ser capaz de encontrar posibles opciones para el modelo, es decir, generar nuevos contenidos (palabras) a partir de ello. "
      ],
      "metadata": {
        "id": "E8r4C9H26UTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Objetivos**\n",
        "**Objetivo general:** Generar texto legible y coherente haciendo uso de una RNN, en base al texto del libro “Frankenstein.”\n",
        "\n",
        "**Objetivos específicos:**\n",
        "1. Hacer uso de una Red neuronal recurrente en base a la librería Pytorch.\n",
        "2. Procesar el texto del libro “Frankenstein” de Mary Shelley, en formato .txt.\n",
        "3. Obtener palabras coherentes y pertenecientes al idioma inglés.\n",
        "4. Definir la importancia del tamaño de la red neuronal, y la importancia de las épocas para el texto generado.\n",
        "5. Apreciar la calidad del texto generado por la red.\n",
        "6. Definir si la red es víctima de underfitting o overfitting y el porqué de ello.\n"
      ],
      "metadata": {
        "id": "Tcx3QvQN6hWe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Materiales y métodos**\n",
        "\n",
        "## *Materiales*\n",
        "\n",
        "los datos del set del libro de Frankenstein; Or, The Modern Prometheus by Mary Wollstonecraft Shelley, \n",
        "\n",
        "Describir este conjunto de datos [[1]](https://www.gutenberg.org/ebooks/42324) (e.g., tipo de secuencia, características).\n",
        "\n",
        "## *Métodos*\n",
        "\n",
        "1. Esquema de metodología (¿qué configuración de RNN utilizarán: seq2seq, ... ?)\n",
        "2. Descripción de los métodos y su implementación en código en forma de narrativa."
      ],
      "metadata": {
        "id": "4rX1w4ZD6mPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocesamiento "
      ],
      "metadata": {
        "id": "oURJi0TEUrlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
        "from string import punctuation"
      ],
      "metadata": {
        "id": "FqLjek2_R0NM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "content = requests.get(\"https://www.gutenberg.org/cache/epub/42324/pg42324.txt\").text\n",
        "open(\"data\\Frankeinstein.txt\", \"w\", encoding=\"utf-8\").write(content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quEmGCl7THAP",
        "outputId": "a3c61873-dea1-4086-adfd-8aba61faa9c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "465593"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 100\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 75\n",
        "# dataset file path\n",
        "FILE_PATH = \"data\\Frankeinstein.txt\"\n",
        "BASENAME = os.path.basename(FILE_PATH)\n",
        "# read the data\n",
        "text = open(FILE_PATH, encoding=\"utf-8\").read()\n",
        "# remove caps, comment this code if you want uppercase characters as well\n",
        "#text = text.lower()\n",
        "# remove punctuation\n",
        "text = text.translate(str.maketrans(\"\", \"\", punctuation))\n",
        "#text = text.ascii_letters + \" .,;'\""
      ],
      "metadata": {
        "id": "i2-_J_81UHSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print some stats\n",
        "n_chars = len(text)\n",
        "vocab = ''.join(sorted(set(text)))\n",
        "print(\"unique_chars:\", vocab)\n",
        "n_unique_chars = len(vocab)\n",
        "print(\"Number of characters:\", n_chars)\n",
        "print(\"Number of unique characters:\", n_unique_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqm_CZnHV2Ui",
        "outputId": "074e23fb-783f-47b2-b6c8-bac2db620e78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unique_chars: \n",
            " 0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzæèéêô﻿\n",
            "Number of characters: 443896\n",
            "Number of unique characters: 70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dictionary that converts characters to integers\n",
        "char2int = {c: i for i, c in enumerate(vocab)}\n",
        "# dictionary that converts integers to characters\n",
        "int2char = {i: c for i, c in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "pQIw1MvEUJrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save these dictionaries for later generation\n",
        "pickle.dump(char2int, open(f\"{BASENAME}-char2int.pickle\", \"wb\"))\n",
        "pickle.dump(int2char, open(f\"{BASENAME}-int2char.pickle\", \"wb\"))"
      ],
      "metadata": {
        "id": "UqVQts7bULpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert all text into integers\n",
        "encoded_text = np.array([char2int[c] for c in text])"
      ],
      "metadata": {
        "id": "EBUrBLSQUOql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# construct tf.data.Dataset object\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)"
      ],
      "metadata": {
        "id": "abTM7vwYUSRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print first 5 characters\n",
        "for char in char_dataset.take(8):\n",
        "    print(char.numpy(), int2char[char.numpy()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19syI7JoUTu0",
        "outputId": "f8c28074-33e5-4e05-a912-14d94dbb60b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "69 ﻿\n",
            "31 T\n",
            "45 h\n",
            "42 e\n",
            "1  \n",
            "27 P\n",
            "55 r\n",
            "52 o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build sequences by batching\n",
        "sequences = char_dataset.batch(2*sequence_length + 1, drop_remainder=True)\n",
        "\n",
        "# print sequences\n",
        "for sequence in sequences.take(2):\n",
        "    print(''.join([int2char[i] for i in sequence.numpy()]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdSMe2rzUVpk",
        "outputId": "52663f93-4348-4d8f-8e91-13e84defe7a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿The Project Gutenberg EBook of Frankenstein by Mary W Shelley\n",
            "\n",
            "This eBook is for the use of anyone anywhere at no cost and with\n",
            "almost no restrictions whatsoever  You may copy it give it away or\n",
            "reuse\n",
            " it under the terms of the Project Gutenberg License included\n",
            "with this eBook or online at wwwgutenbergorg\n",
            "\n",
            "\n",
            "Title Frankenstein\n",
            "       or The Modern Prometheus\n",
            "\n",
            "Author Mary W Shelley\n",
            "\n",
            "Release Date Marc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_sample(sample):\n",
        "    # example :\n",
        "    # sequence_length is 10\n",
        "    # sample is \"python is a great pro\" (21 length)\n",
        "    # ds will equal to ('python is ', 'a') encoded as integers\n",
        "    ds = tf.data.Dataset.from_tensors((sample[:sequence_length], sample[sequence_length]))\n",
        "    for i in range(1, (len(sample)-1) // 2):\n",
        "        # first (input_, target) will be ('ython is a', ' ')\n",
        "        # second (input_, target) will be ('thon is a ', 'g')\n",
        "        # third (input_, target) will be ('hon is a g', 'r')\n",
        "        # and so on\n",
        "        input_ = sample[i: i+sequence_length]\n",
        "        target = sample[i+sequence_length]\n",
        "        # extend the dataset with these samples by concatenate() method\n",
        "        other_ds = tf.data.Dataset.from_tensors((input_, target))\n",
        "        ds = ds.concatenate(other_ds)\n",
        "    return ds\n",
        "\n",
        "# prepare inputs and targets\n",
        "dataset = sequences.flat_map(split_sample)"
      ],
      "metadata": {
        "id": "eiPeMX5PUXmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_samples(input_, target):\n",
        "    # onehot encode the inputs and the targets\n",
        "    # Example:\n",
        "    # if character 'd' is encoded as 3 and n_unique_chars = 5\n",
        "    # result should be the vector: [0, 0, 0, 1, 0], since 'd' is the 4th character\n",
        "    return tf.one_hot(input_, n_unique_chars), tf.one_hot(target, n_unique_chars)\n",
        "\n",
        "dataset = dataset.map(one_hot_samples)"
      ],
      "metadata": {
        "id": "RWa3pp7RUZ7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print first 2 samples\n",
        "for element in dataset.take(2):\n",
        "    print(\"Input:\", ''.join([int2char[np.argmax(char_vector)] for char_vector in element[0].numpy()]))\n",
        "    print(\"Target:\", int2char[np.argmax(element[1].numpy())])\n",
        "    print(\"Input shape:\", element[0].shape)\n",
        "    print(\"Target shape:\", element[1].shape)\n",
        "    print(\"=\"*50, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wq8GLzP2UdPr",
        "outputId": "410c128b-d6c3-4625-e9fd-1c1c3b334942"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: ﻿The Project Gutenberg EBook of Frankenstein by Mary W Shelley\n",
            "\n",
            "This eBook is for the use of anyone \n",
            "Target: a\n",
            "Input shape: (100, 70)\n",
            "Target shape: (70,)\n",
            "================================================== \n",
            "\n",
            "Input: The Project Gutenberg EBook of Frankenstein by Mary W Shelley\n",
            "\n",
            "This eBook is for the use of anyone a\n",
            "Target: n\n",
            "Input shape: (100, 70)\n",
            "Target shape: (70,)\n",
            "================================================== \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# repeat, shuffle and batch the dataset\n",
        "ds = dataset.repeat().shuffle(1024).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "hrpq7xhJUesl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U numpy==1.18.5"
      ],
      "metadata": {
        "id": "k8VUpMHSUhE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    LSTM(256, input_shape=(sequence_length, n_unique_chars), return_sequences=True),\n",
        "    Dropout(0.3),\n",
        "    LSTM(256),\n",
        "    Dense(n_unique_chars, activation=\"softmax\"),])"
      ],
      "metadata": {
        "id": "rTP7cHVrUhxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    LSTM(256, input_shape=(sequence_length, n_unique_chars), return_sequences=True),\n",
        "    Dropout(0.3),\n",
        "    LSTM(256),\n",
        "    Dense(n_unique_chars, activation=\"softmax\"),])"
      ],
      "metadata": {
        "id": "yMLgb5-aUkfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the model path\n",
        "model_weights_path = f\"results/{BASENAME}-{sequence_length}.h5\"\n",
        "model.summary()\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhaHdhCSWa0J",
        "outputId": "2d5b5e8d-e7ec-4893-cba2-ee63608a1c22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_2 (LSTM)               (None, 100, 256)          334848    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 100, 256)          0         \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 256)               525312    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 70)                17990     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 878,150\n",
            "Trainable params: 878,150\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## entrenamiento\n"
      ],
      "metadata": {
        "id": "qumjFmZXUl_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make results folder if does not exist yet\n",
        "if not os.path.isdir(\"results\"):\n",
        "    os.mkdir(\"results\")\n",
        "# train the model\n",
        "model.fit(ds, steps_per_epoch=(len(encoded_text) - sequence_length) // BATCH_SIZE, epochs=EPOCHS)\n",
        "# save the model\n",
        "model.save(model_weights_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7P574EIVVX8",
        "outputId": "2f21cd73-9a76-4d37-bf52-d3dc53f62646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/75\n",
            "1733/1733 [==============================] - 127s 73ms/step - loss: 1.9166 - accuracy: 0.4309\n",
            "Epoch 2/75\n",
            "1733/1733 [==============================] - 125s 72ms/step - loss: 1.6347 - accuracy: 0.5091\n",
            "Epoch 3/75\n",
            "1733/1733 [==============================] - 124s 72ms/step - loss: 1.4752 - accuracy: 0.5557\n",
            "Epoch 4/75\n",
            "1733/1733 [==============================] - 124s 72ms/step - loss: 1.3728 - accuracy: 0.5844\n",
            "Epoch 5/75\n",
            "1733/1733 [==============================] - 124s 72ms/step - loss: 1.2946 - accuracy: 0.6057\n",
            "Epoch 6/75\n",
            "1733/1733 [==============================] - 122s 70ms/step - loss: 1.2222 - accuracy: 0.6234\n",
            "Epoch 7/75\n",
            "1733/1733 [==============================] - 122s 70ms/step - loss: 1.1628 - accuracy: 0.6390\n",
            "Epoch 8/75\n",
            "1733/1733 [==============================] - 124s 72ms/step - loss: 1.1146 - accuracy: 0.6535\n",
            "Epoch 9/75\n",
            "1733/1733 [==============================] - 125s 72ms/step - loss: 1.0710 - accuracy: 0.6654\n",
            "Epoch 10/75\n",
            "1733/1733 [==============================] - 125s 72ms/step - loss: 1.0335 - accuracy: 0.6757\n",
            "Epoch 11/75\n",
            "1733/1733 [==============================] - 125s 72ms/step - loss: 1.0014 - accuracy: 0.6854\n",
            "Epoch 12/75\n",
            "1733/1733 [==============================] - 125s 72ms/step - loss: 0.9703 - accuracy: 0.6932\n",
            "Epoch 13/75\n",
            "1733/1733 [==============================] - 125s 72ms/step - loss: 0.9425 - accuracy: 0.7019\n",
            "Epoch 14/75\n",
            "1733/1733 [==============================] - 126s 72ms/step - loss: 0.9151 - accuracy: 0.7100\n",
            "Epoch 15/75\n",
            "1733/1733 [==============================] - 125s 72ms/step - loss: 0.8944 - accuracy: 0.7156\n",
            "Epoch 16/75\n",
            "1733/1733 [==============================] - 125s 72ms/step - loss: 0.8737 - accuracy: 0.7210\n",
            "Epoch 17/75\n",
            "1733/1733 [==============================] - 125s 72ms/step - loss: 0.8546 - accuracy: 0.7277\n",
            "Epoch 18/75\n",
            "1733/1733 [==============================] - 125s 72ms/step - loss: 0.8379 - accuracy: 0.7311\n",
            "Epoch 19/75\n",
            "1733/1733 [==============================] - 126s 73ms/step - loss: 0.8235 - accuracy: 0.7350\n",
            "Epoch 20/75\n",
            "1733/1733 [==============================] - 127s 73ms/step - loss: 0.8100 - accuracy: 0.7393\n",
            "Epoch 21/75\n",
            "1733/1733 [==============================] - 126s 73ms/step - loss: 0.7930 - accuracy: 0.7444\n",
            "Epoch 22/75\n",
            "1733/1733 [==============================] - 127s 73ms/step - loss: 0.7815 - accuracy: 0.7474\n",
            "Epoch 23/75\n",
            "1733/1733 [==============================] - 127s 73ms/step - loss: 0.7706 - accuracy: 0.7509\n",
            "Epoch 24/75\n",
            "1733/1733 [==============================] - 126s 73ms/step - loss: 0.7601 - accuracy: 0.7531\n",
            "Epoch 25/75\n",
            "1733/1733 [==============================] - 126s 73ms/step - loss: 0.7467 - accuracy: 0.7577\n",
            "Epoch 26/75\n",
            "1733/1733 [==============================] - 126s 73ms/step - loss: 0.7380 - accuracy: 0.7603\n",
            "Epoch 27/75\n",
            "1733/1733 [==============================] - 126s 73ms/step - loss: 0.7261 - accuracy: 0.7639\n",
            "Epoch 28/75\n",
            "1733/1733 [==============================] - 126s 73ms/step - loss: 0.7178 - accuracy: 0.7655\n",
            "Epoch 29/75\n",
            "1733/1733 [==============================] - 127s 73ms/step - loss: 0.7109 - accuracy: 0.7678\n",
            "Epoch 30/75\n",
            "1733/1733 [==============================] - 126s 73ms/step - loss: 0.6992 - accuracy: 0.7713\n",
            "Epoch 31/75\n",
            "1733/1733 [==============================] - 125s 72ms/step - loss: 0.6972 - accuracy: 0.7722\n",
            "Epoch 32/75\n",
            "1733/1733 [==============================] - 126s 73ms/step - loss: 0.6857 - accuracy: 0.7756\n",
            "Epoch 33/75\n",
            "1733/1733 [==============================] - 126s 73ms/step - loss: 0.6769 - accuracy: 0.7776\n",
            "Epoch 34/75\n",
            "1733/1733 [==============================] - 126s 73ms/step - loss: 0.6734 - accuracy: 0.7778\n",
            "Epoch 35/75\n",
            "1733/1733 [==============================] - 126s 73ms/step - loss: 0.6651 - accuracy: 0.7808\n",
            "Epoch 36/75\n",
            "1733/1733 [==============================] - 125s 72ms/step - loss: 0.6603 - accuracy: 0.7828\n",
            "Epoch 37/75\n",
            " 557/1733 [========>.....................] - ETA: 1:24 - loss: 0.6602 - accuracy: 0.7829"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import tqdm\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, Activation\n",
        "import os\n",
        "\n",
        "sequence_length = 100\n",
        "# dataset file path\n",
        "FILE_PATH = \"/content/data\\Frankeinstein.txt\"\n",
        "# FILE_PATH = \"data/python_code.py\"\n",
        "BASENAME = os.path.basename(FILE_PATH)"
      ],
      "metadata": {
        "id": "ce0phYkCVZys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = \"disconfort\""
      ],
      "metadata": {
        "id": "XwUyOkmjVaxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load vocab dictionaries\n",
        "char2int = pickle.load(open(f\"{BASENAME}-char2int.pickle\", \"rb\"))\n",
        "int2char = pickle.load(open(f\"{BASENAME}-int2char.pickle\", \"rb\"))\n",
        "vocab_size = len(char2int)"
      ],
      "metadata": {
        "id": "wyie6yF3Vclr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# building the model\n",
        "model = Sequential([\n",
        "    LSTM(256, input_shape=(sequence_length, vocab_size), return_sequences=True),\n",
        "    Dropout(0.3),\n",
        "    LSTM(256),\n",
        "    Dense(vocab_size, activation=\"softmax\"),])"
      ],
      "metadata": {
        "id": "9TiDD6eCVeK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the optimal weights\n",
        "model.load_weights(f\"results/{BASENAME}-{sequence_length}.h5\")"
      ],
      "metadata": {
        "id": "6-uvV3EqVhPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = seed\n",
        "n_chars = 600\n",
        "# generate 400 characters\n",
        "generated = \"\"\n",
        "for i in tqdm.tqdm(range(n_chars), \"Generating text\"):\n",
        "    # make the input sequence\n",
        "    X = np.zeros((1, sequence_length, vocab_size))\n",
        "    for t, char in enumerate(seed):\n",
        "        X[0, (sequence_length - len(seed)) + t, char2int[char]] = 1\n",
        "    # predict the next character\n",
        "    predicted = model.predict(X, verbose=0)[0]\n",
        "    # converting the vector to an integer\n",
        "    next_index = np.argmax(predicted)\n",
        "    # converting the integer to a character\n",
        "    next_char = int2char[next_index]\n",
        "    # add the character to results\n",
        "    generated += next_char\n",
        "    # shift seed and the predicted character\n",
        "    seed = seed[1:] + next_char\n",
        "\n",
        "print(\"Seed:\", s)\n",
        "print(\"Generated text:\")\n",
        "print(generated)"
      ],
      "metadata": {
        "id": "FirjJfefVjHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DBRVR9i6VmRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Resultados**\n",
        "(300-800 palabras, incluir mínimo 2 figuras)."
      ],
      "metadata": {
        "id": "6s3y9-yUSCL9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Discusión**\n",
        "El modelo RNN generador de texto, busca encontrar y recordar información de time steps ya que el efecto del número de capas internas que una red neuronal recurrente requiere tiene que ver con la salida de la capa en un time step concreto, y viene determinada por los datos de entrada que la capa ha visto hasta ese momento. \n",
        "\n",
        "De esta forma la RNN pueda realizar predicciones de lo que sucederá o generará en un futuro, que serían palabras o en el mejor de los casos oraciones coherentes y entendibles en el lenguaje humano a partir de los datos del set del libro de Frankenstein; Or, The Modern Prometheus by Mary Wollstonecraft Shelley, con el fin de que el texto del libro sea procesado y pueda recordarlo para así asociar conceptos con las nuevas frases que va analizando porque ya las ha “visto”.\n",
        "\n",
        "Para el modelo se utilizó un tamaño de batch de 256 y se está entrenado por 30 épocas. Y solo de esta forma demostró un buen trabajo de la red en el preprocesamiento y la fase de prueba.\n",
        "\n",
        "En este modelo no presento tener en cuenta demasiada información de sus datos y / o conocimientos previos (overfitting), sin embargo, Sí nuestros datos de entrenamiento son muy pocos nuestra máquina no será capaz de generalizar el conocimiento y por eso se genera underfitting.\n",
        "\n",
        "En comparación con otras literaturas asociadas una gran diferencia la podíamos notar en las épocas que utilizaban en el entrenamiento, algunas utilizan 100, 200,80 cuando mínimo, pero nuestro modelo con 30 y 10 con las que fue probado si funciona bien. Otra de las diferencias que pudimos notar es que su clasificación de su modelo automático es además capaz de discernir correctamente entre texto con coherencia y cohesión.\n",
        "Es probable que, si ponemos en nuestro modelo un primer token para iniciar la secuencia, buscando una frase en su completitud.\n",
        "\n",
        "El modelo finalmente tiene algunos detalles aun y se le podrían agregar mas cosas para lograr una exactitud mas precisa al momento de generar palabras. Sin embargo, aun así funciona bastante bien y los resultados son satisfactorios, porque el texto generado es principalmente legible.\n",
        "\n"
      ],
      "metadata": {
        "id": "AEt8aII8SPu6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Conclusiones**\n",
        "Con tus propias palabras describe si se cumplen o no los objetivos en base al análisis de los resultados\n",
        "\n",
        "•\t¿El modelo genera texto? \n",
        "\n",
        "•\t¿Cómo lo justificaron y cuáles son sus limitaciones? \n",
        "\n",
        "•\t¿Cuáles son sus perspectivas?\n",
        "\n",
        "En este de modelo propuesto se mostró que es capaz de generar palabras entendibles y en ingles, la coherencia en no es una su punto fuerte sin embargo  con estructuras como la RNN (Recurrent neuronal networks) y sus resultados fueron comparables con otras de las literaturas asociadas debido a.... \n"
      ],
      "metadata": {
        "id": "8Gb2ac0VSryX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. Referencias**\n",
        "\n",
        "[1] Frankenstein; Or, The Modern Prometheus by Mary Wollstonecraft Shelley. (1818). Project Gutenberg. Recuperado 8 de octubre de 2022, de https://www.gutenberg.org/ebooks/42324"
      ],
      "metadata": {
        "id": "cxRqAiW5sG_m"
      }
    }
  ]
}