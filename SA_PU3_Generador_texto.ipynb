{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dayaanaly/SA-PU3-Modelo_recurrentes/blob/Chris/SA_PU3_Generador_texto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Práctica #3: Modelo para generación de texto**\n",
        "\n",
        "*Centro Universitario de Ciencias Exactas e Ingenierías*\n",
        "\n",
        "*División de Tecnologías para la Integración Ciber-Humana*\n",
        "\n",
        "*Ingeniería Biomédica*\n",
        "\n",
        "<br>\n",
        "\n",
        "*Mtra. Sofía Alejandra Aguilar Valdez*\n",
        "\n",
        "14 de octubre de 2022"
      ],
      "metadata": {
        "id": "nVx-kou4pjFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Información del equipo**\n",
        "\n",
        "```NOMBRES:```\n",
        "\n",
        "Esteban Gómez Fuentes\n",
        "\n",
        "Christopher William Sanchez Rodriguez\n",
        "\n",
        "Dayana Analy Pacheco Bañuelos\n",
        "\n",
        "\n",
        "```CÓDIGOS:```\n",
        "\n",
        "217593587\n",
        "\n",
        "217535226\n",
        "\n",
        "215658088\n",
        "\n",
        "```LINK REPOSITORIO:```\n",
        "https://github.com/Dayaanaly/SA-PU3-Modelo_recurrentes\n",
        "\n"
      ],
      "metadata": {
        "id": "dQNzG9Wm4ZQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Contenido**\n",
        "\n",
        "\n",
        "\n",
        "1.   Resumen\n",
        "2.   Marco teórico\n",
        "3.   Objetivos\n",
        "4.   Materiales y métodos\n",
        "5.   Resultados\n",
        "6.   Discusión\n",
        "7.   Conclusiones\n",
        "8.   Referencias\n",
        "\n"
      ],
      "metadata": {
        "id": "QQ_tQMMJpude"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Resumen**\n",
        "Esta práctica consiste en la creación de una red neuronal recurrente (RNN) que sea capaz de procesar el texto del libro “Frankenstein” para utilizarlo como base de datos. En base a estos datos, el modelo debe de generar un párrafo de texto que en teoría debe de tener cierta relación con el contenido del libro. En cuanto a los resultados lo que se busca primordialmente es que el texto generado contenga palabras con coherencia y que formen parte de nuestro diccionario. La redacción, coherencia y lógica de este texto generado queda en un segundo plano, ya que esos son elementos un poco más complejos de obtener, y en cierta medida nuestro modelo de red no tiene forma de interpretar estos parámetros."
      ],
      "metadata": {
        "id": "anvzyOk06GH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Marco teórico**\n",
        "(300-800 palabras)\n",
        "\n",
        "Una Red de Neuronas Artificial (RNA) es un modelo matemático de procesamiento, distribuido paralelamente, en forma de grafo dirigido, simulando el comportamiento biológico de las neuronas y la estructura del cerebro. \n",
        "\n",
        "Las Redes de Neuronas Recurrentes o en inglés, Recurrent Neural Networks (RNN) son modelos de redes de neuronas artificiales (RNAs), donde las conexiones entre unidades forman un ciclo dirigido. Específicamente, un ciclo dirigido es una secuencia en la que la caminata a lo largo de los vértices y bordes está completamente determinada por el conjunto de bordes utilizados y, por lo tanto, tiene una apariencia de un orden específico. Este tipo de redes neuronales está especializado en el procesamiento de datos secuenciales o series temporales como es el caso del reconocimiento de voz y escritura. \n",
        "\n",
        "![](https://www.simplilearn.com/ice9/free_resources_article_thumb/Simple_Recurrent_Neural_Network.png) \n",
        "\n",
        "Una RNN basada en caracteres y de esta manera también podemos usar el caso al mismo tiempo para mostrar el uso de datos de texto. En este ejemplo se entrena un modelo de red neuronal para predecir el siguiente carácter a partir de una secuencia de carácteres. Con este modelo intencionadamente simple, se consigue generar secuencias de texto más largas llamando al modelo repetidamente.\n",
        "\n",
        "El reconocimiento de texto es una de las partes más importantes de una RNN, ya que partir de esta característica y la creatividad, podemos obtener la generación y de algún modo, puede ser considerada un paso previo.  \n",
        "\n",
        "El texto se le conoce al conjunto de frases y palabras coherentes y ordenadas que permiten ser interpretadas y transmiten las ideas de un autor. Su extensión es variable y corresponde a un todo comprensible que tiene una finalidad comunicativa en un contexto dado.  \n",
        "\n",
        "La generación se puede definir como la acción que consiste en producir o crear una cosa, y si la aplicamos a términos de la inteligencia artificial (IA), podría describirse como la capacidad de crear contenido indistinguible del aprendido (original) de manera global pero diferente a cada elemento de manera individual. Un ejemplo fácil en el campo de la visión Artificial sería poder “dibujar” un perro en un campo de hierba, habiendo visto únicamente campos de hierba y perros sobre arena por separado. Si juntáramos las tres imágenes en una misma carpeta, no se podría decir con certeza cuál de las tres ha sido generada por un sistema (ficticio) de creación de imágenes.  \n",
        "\n",
        "Para la creación de texto gramaticalmente correcto en el mismo dominio del original de manera global, pero con diferente significado o contenido a cada uno de los textos de manera individual. Se quiere una técnica de generación de texto fácil de ajustar y que combine una técnica no compleja de clasificación con una búsqueda sobre las soluciones del anterior, consiguiendo la unión de diferentes datos secuenciales relativamente sencillos en un sistema de mayor complejidad.  \n",
        "Un algoritmo de búsqueda debería ser capaz de encontrar posibles opciones para el modelo, es decir, generar nuevos contenidos (palabras) a partir de ello. "
      ],
      "metadata": {
        "id": "E8r4C9H26UTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Objetivos**\n",
        "Objetivo general y objetivos específicos.\n",
        "\n",
        "\n",
        "\n",
        "Sean claros al indicar el objetivo general y los objetivos específicos.\n",
        "\n",
        "El objetivo general engloba los objetivos específicos. Por ejemplo:\n",
        "\n",
        "Objetivo general: hacer un sándwich.\n",
        "\n",
        "Objetivos específicos:\n",
        "\n",
        "definir ingredientes para el sándwich.\n",
        "identificar cuáles tengo en casa.\n",
        "construir el sándwich con los ingredientes según una receta.\n",
        "Les recomiendo utilizar la taxonomía de Bloom al redactar sus objetivos específicos para que se entienda en qué nivel de conocimiento empiezan y en cuál terminan (e.g., definir -> clasificar -> interpretar)."
      ],
      "metadata": {
        "id": "Tcx3QvQN6hWe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Materiales y métodos**\n",
        "\n",
        "## *Materiales*\n",
        "\n",
        "Describir este conjunto de datos [[1]](https://www.gutenberg.org/ebooks/42324) (e.g., tipo de secuencia, características).\n",
        "\n",
        "## *Métodos*\n",
        "\n",
        "1. Esquema de metodología (¿qué configuración de RNN utilizarán: seq2seq, ... ?)\n",
        "2. Descripción de los métodos y su implementación en código en forma de narrativa."
      ],
      "metadata": {
        "id": "4rX1w4ZD6mPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocesamiento "
      ],
      "metadata": {
        "id": "oURJi0TEUrlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
        "from string import punctuation"
      ],
      "metadata": {
        "id": "FqLjek2_R0NM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "content = requests.get(\"https://www.gutenberg.org/cache/epub/42324/pg42324.txt\").text\n",
        "open(\"data\\Frankeinstein.txt\", \"w\", encoding=\"utf-8\").write(content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quEmGCl7THAP",
        "outputId": "15357326-f05f-4827-9f31-cedec051ff26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "465593"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 100\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 30\n",
        "# dataset file path\n",
        "FILE_PATH = \"data\\Frankeinstein.txt\"\n",
        "BASENAME = os.path.basename(FILE_PATH)\n",
        "# read the data\n",
        "text = open(FILE_PATH, encoding=\"utf-8\").read()\n",
        "# remove caps, comment this code if you want uppercase characters as well\n",
        "#text = text.lower()\n",
        "# remove punctuation\n",
        "text = text.translate(str.maketrans(\"\", \"\", punctuation))\n",
        "#text = text.ascii_letters + \" .,;'\""
      ],
      "metadata": {
        "id": "i2-_J_81UHSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print some stats\n",
        "n_chars = len(text)\n",
        "vocab = ''.join(sorted(set(text)))\n",
        "print(\"unique_chars:\", vocab)\n",
        "n_unique_chars = len(vocab)\n",
        "print(\"Number of characters:\", n_chars)\n",
        "print(\"Number of unique characters:\", n_unique_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqm_CZnHV2Ui",
        "outputId": "d4f9cf13-e27f-4aab-8f21-7448fba94fd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unique_chars: \n",
            " 0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzæèéêô﻿\n",
            "Number of characters: 443896\n",
            "Number of unique characters: 70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dictionary that converts characters to integers\n",
        "char2int = {c: i for i, c in enumerate(vocab)}\n",
        "# dictionary that converts integers to characters\n",
        "int2char = {i: c for i, c in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "pQIw1MvEUJrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save these dictionaries for later generation\n",
        "pickle.dump(char2int, open(f\"{BASENAME}-char2int.pickle\", \"wb\"))\n",
        "pickle.dump(int2char, open(f\"{BASENAME}-int2char.pickle\", \"wb\"))"
      ],
      "metadata": {
        "id": "UqVQts7bULpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert all text into integers\n",
        "encoded_text = np.array([char2int[c] for c in text])"
      ],
      "metadata": {
        "id": "EBUrBLSQUOql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# construct tf.data.Dataset object\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)"
      ],
      "metadata": {
        "id": "abTM7vwYUSRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print first 5 characters\n",
        "for char in char_dataset.take(8):\n",
        "    print(char.numpy(), int2char[char.numpy()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19syI7JoUTu0",
        "outputId": "0629d223-dfd1-475a-eeaa-0025d2227c80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "69 ﻿\n",
            "31 T\n",
            "45 h\n",
            "42 e\n",
            "1  \n",
            "27 P\n",
            "55 r\n",
            "52 o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build sequences by batching\n",
        "sequences = char_dataset.batch(2*sequence_length + 1, drop_remainder=True)\n",
        "\n",
        "# print sequences\n",
        "for sequence in sequences.take(2):\n",
        "    print(''.join([int2char[i] for i in sequence.numpy()]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdSMe2rzUVpk",
        "outputId": "62636f3e-adc9-4799-dd36-260606ebf0bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿The Project Gutenberg EBook of Frankenstein by Mary W Shelley\n",
            "\n",
            "This eBook is for the use of anyone anywhere at no cost and with\n",
            "almost no restrictions whatsoever  You may copy it give it away or\n",
            "reuse\n",
            " it under the terms of the Project Gutenberg License included\n",
            "with this eBook or online at wwwgutenbergorg\n",
            "\n",
            "\n",
            "Title Frankenstein\n",
            "       or The Modern Prometheus\n",
            "\n",
            "Author Mary W Shelley\n",
            "\n",
            "Release Date Marc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_sample(sample):\n",
        "    # example :\n",
        "    # sequence_length is 10\n",
        "    # sample is \"python is a great pro\" (21 length)\n",
        "    # ds will equal to ('python is ', 'a') encoded as integers\n",
        "    ds = tf.data.Dataset.from_tensors((sample[:sequence_length], sample[sequence_length]))\n",
        "    for i in range(1, (len(sample)-1) // 2):\n",
        "        # first (input_, target) will be ('ython is a', ' ')\n",
        "        # second (input_, target) will be ('thon is a ', 'g')\n",
        "        # third (input_, target) will be ('hon is a g', 'r')\n",
        "        # and so on\n",
        "        input_ = sample[i: i+sequence_length]\n",
        "        target = sample[i+sequence_length]\n",
        "        # extend the dataset with these samples by concatenate() method\n",
        "        other_ds = tf.data.Dataset.from_tensors((input_, target))\n",
        "        ds = ds.concatenate(other_ds)\n",
        "    return ds\n",
        "\n",
        "# prepare inputs and targets\n",
        "dataset = sequences.flat_map(split_sample)"
      ],
      "metadata": {
        "id": "eiPeMX5PUXmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_samples(input_, target):\n",
        "    # onehot encode the inputs and the targets\n",
        "    # Example:\n",
        "    # if character 'd' is encoded as 3 and n_unique_chars = 5\n",
        "    # result should be the vector: [0, 0, 0, 1, 0], since 'd' is the 4th character\n",
        "    return tf.one_hot(input_, n_unique_chars), tf.one_hot(target, n_unique_chars)\n",
        "\n",
        "dataset = dataset.map(one_hot_samples)"
      ],
      "metadata": {
        "id": "RWa3pp7RUZ7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print first 2 samples\n",
        "for element in dataset.take(2):\n",
        "    print(\"Input:\", ''.join([int2char[np.argmax(char_vector)] for char_vector in element[0].numpy()]))\n",
        "    print(\"Target:\", int2char[np.argmax(element[1].numpy())])\n",
        "    print(\"Input shape:\", element[0].shape)\n",
        "    print(\"Target shape:\", element[1].shape)\n",
        "    print(\"=\"*50, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wq8GLzP2UdPr",
        "outputId": "26cf74e0-6dda-4461-a576-1fe5013b8d25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: ﻿The Project Gutenberg EBook of Frankenstein by Mary W Shelley\n",
            "\n",
            "This eBook is for the use of anyone \n",
            "Target: a\n",
            "Input shape: (100, 70)\n",
            "Target shape: (70,)\n",
            "================================================== \n",
            "\n",
            "Input: The Project Gutenberg EBook of Frankenstein by Mary W Shelley\n",
            "\n",
            "This eBook is for the use of anyone a\n",
            "Target: n\n",
            "Input shape: (100, 70)\n",
            "Target shape: (70,)\n",
            "================================================== \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# repeat, shuffle and batch the dataset\n",
        "ds = dataset.repeat().shuffle(1024).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "hrpq7xhJUesl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U numpy==1.18.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "k8VUpMHSUhE1",
        "outputId": "c15aac89-212c-4ded-9b31-17933729e6ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.18.5\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 1.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.18.5 which is incompatible.\n",
            "tensorflow 2.9.2 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n",
            "plotnine 0.8.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n",
            "jaxlib 0.3.20+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "jax 0.3.21 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "cmdstanpy 1.0.7 requires numpy>=1.21, but you have numpy 1.18.5 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.18.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    LSTM(256, input_shape=(sequence_length, n_unique_chars), return_sequences=True),\n",
        "    Dropout(0.3),\n",
        "    LSTM(256),\n",
        "    Dense(n_unique_chars, activation=\"softmax\"),])"
      ],
      "metadata": {
        "id": "rTP7cHVrUhxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    LSTM(256, input_shape=(sequence_length, n_unique_chars), return_sequences=True),\n",
        "    Dropout(0.3),\n",
        "    LSTM(256),\n",
        "    Dense(n_unique_chars, activation=\"softmax\"),])"
      ],
      "metadata": {
        "id": "yMLgb5-aUkfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the model path\n",
        "model_weights_path = f\"results/{BASENAME}-{sequence_length}.h5\"\n",
        "model.summary()\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhaHdhCSWa0J",
        "outputId": "a9928fc4-eb09-441f-8fdd-aab7ec1837f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_2 (LSTM)               (None, 100, 256)          334848    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 100, 256)          0         \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 256)               525312    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 70)                17990     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 878,150\n",
            "Trainable params: 878,150\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## entrenamiento\n"
      ],
      "metadata": {
        "id": "qumjFmZXUl_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make results folder if does not exist yet\n",
        "if not os.path.isdir(\"results\"):\n",
        "    os.mkdir(\"results\")\n",
        "# train the model\n",
        "model.fit(ds, steps_per_epoch=(len(encoded_text) - sequence_length) // BATCH_SIZE, epochs=EPOCHS)\n",
        "# save the model\n",
        "model.save(model_weights_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7P574EIVVX8",
        "outputId": "40e6c5a8-aacf-41e0-f374-5cfec3b61ec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "  14/1733 [..............................] - ETA: 1:40:20 - loss: 3.6785 - accuracy: 0.1247"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import tqdm\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, Activation\n",
        "import os\n",
        "\n",
        "sequence_length = 100\n",
        "# dataset file path\n",
        "FILE_PATH = \"/content/data\\Frankeinstein.txt\"\n",
        "# FILE_PATH = \"data/python_code.py\"\n",
        "BASENAME = os.path.basename(FILE_PATH)"
      ],
      "metadata": {
        "id": "ce0phYkCVZys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = \"disconfort\""
      ],
      "metadata": {
        "id": "XwUyOkmjVaxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load vocab dictionaries\n",
        "char2int = pickle.load(open(f\"{BASENAME}-char2int.pickle\", \"rb\"))\n",
        "int2char = pickle.load(open(f\"{BASENAME}-int2char.pickle\", \"rb\"))\n",
        "vocab_size = len(char2int)"
      ],
      "metadata": {
        "id": "wyie6yF3Vclr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# building the model\n",
        "model = Sequential([\n",
        "    LSTM(256, input_shape=(sequence_length, vocab_size), return_sequences=True),\n",
        "    Dropout(0.3),\n",
        "    LSTM(256),\n",
        "    Dense(vocab_size, activation=\"softmax\"),])"
      ],
      "metadata": {
        "id": "9TiDD6eCVeK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the optimal weights\n",
        "model.load_weights(f\"results/{BASENAME}-{sequence_length}.h5\")"
      ],
      "metadata": {
        "id": "6-uvV3EqVhPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = seed\n",
        "n_chars = 5000\n",
        "# generate 400 characters\n",
        "generated = \"\"\n",
        "for i in tqdm.tqdm(range(n_chars), \"Generating text\"):\n",
        "    # make the input sequence\n",
        "    X = np.zeros((1, sequence_length, vocab_size))\n",
        "    for t, char in enumerate(seed):\n",
        "        X[0, (sequence_length - len(seed)) + t, char2int[char]] = 1\n",
        "    # predict the next character\n",
        "    predicted = model.predict(X, verbose=0)[0]\n",
        "    # converting the vector to an integer\n",
        "    next_index = np.argmax(predicted)\n",
        "    # converting the integer to a character\n",
        "    next_char = int2char[next_index]\n",
        "    # add the character to results\n",
        "    generated += next_char\n",
        "    # shift seed and the predicted character\n",
        "    seed = seed[1:] + next_char\n",
        "\n",
        "print(\"Seed:\", s)\n",
        "print(\"Generated text:\")\n",
        "print(generated)"
      ],
      "metadata": {
        "id": "FirjJfefVjHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DBRVR9i6VmRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Resultados**\n",
        "(300-800 palabras, incluir mínimo 2 figuras)."
      ],
      "metadata": {
        "id": "6s3y9-yUSCL9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Discusión**\n",
        "El modelo RNN generador de texto, busca encontrar y recordar información de time steps ya que el efecto del número de capas internas que una red neuronal recurrente requiere tiene que ver con la salida de la capa en un time step concreto, y viene determinada por los datos de entrada que la capa ha visto hasta ese momento. \n",
        "\n",
        "De esta forma la RNN pueda realizar predicciones de lo que sucederá o generará en un futuro, que serían palabras o en el mejor de los casos oraciones coherentes y entendibles en el lenguaje humano a partir de los datos del set del libro de Frankenstein; Or, The Modern Prometheus by Mary Wollstonecraft Shelley, con el fin de que el texto del libro sea procesado y pueda recordarlo para así asociar conceptos con las nuevas frases que va analizando porque ya las ha “visto”.\n",
        "\n",
        "Para el modelo se utilizó un tamaño de batch de _ y se está entrenado por _épocas.\n",
        "\n",
        "En este modelo…( Underfitting, overfitting, o no y el porqué)\n",
        "\n",
        "•\tComparación de sus resultados con los de literatura asociada.\n"
      ],
      "metadata": {
        "id": "AEt8aII8SPu6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Conclusiones**\n",
        "Con tus propias palabras describe si se cumplen o no los objetivos en base al análisis de los resultados\n",
        "\n",
        "•\t¿El modelo genera texto? \n",
        "\n",
        "•\t¿Cómo lo justificaron y cuáles son sus limitaciones? \n",
        "\n",
        "•\t¿Cuáles son sus perspectivas?\n",
        "\n",
        "En este de modelo propuesto se mostró que es capaz de ..... con estructuras como la RNN (Recurrent neuronal networks) y sus resultados fueron comparables con otras de las literaturas asociadas debido a.... \n"
      ],
      "metadata": {
        "id": "8Gb2ac0VSryX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. Referencias**\n",
        "\n",
        "[1] Frankenstein; Or, The Modern Prometheus by Mary Wollstonecraft Shelley. (1818). Project Gutenberg. Recuperado 8 de octubre de 2022, de https://www.gutenberg.org/ebooks/42324"
      ],
      "metadata": {
        "id": "cxRqAiW5sG_m"
      }
    }
  ]
}